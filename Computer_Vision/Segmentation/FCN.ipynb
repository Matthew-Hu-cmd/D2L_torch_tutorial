{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全卷积神经网络  \n",
    "fully convolutional network，FCN  \n",
    "通过使用转置卷积，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构造：\n",
    "* 基本的卷积神经网络提取特征  \n",
    "* 1x1卷积将通道数变换为类别个数  \n",
    "* 最终转置卷积将特征图的高和宽变化为输入图像尺寸，因此，模型输出与输入图像高宽相同，并且最终输出的通道包含该空间位置像素类别的预测。  \n",
    "![最简单的可以用于分割的模型](../img/fcn.svg) . \n",
    "使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征，并将该网络记为pretrained_net。 ResNet-18模型的最后几层包括全局平均汇聚层和全连接层，然而全卷积网络中不需要它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
    "list(pretrained_net.children())[-3:]\n",
    "#children就是吧所有的层拿出来仔细看看，这边我们选取了最后三层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全卷积网络net:它复制了ResNet-18中大部分的预训练层，除了最后的全局平均汇聚层和最接近输出的全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 10, 15])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建立一个全卷积网络的实例，把除了最后一层的全部拿出来\n",
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])\n",
    "# 给定高度为320和宽度为480的输入，net的前向传播将输入的高和宽减小至原来的1/32 \n",
    "X = torch.rand(size=(1, 3, 320, 480))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像分割数据集的输入channel给定之后经过预训练的网络，得到了512, 10, 15这样的输出  \n",
    "设计能够实现图像分割任务的输出：  \n",
    "* 1x1的卷积将数据集的通道数转换成21类  \n",
    "* 利用转置卷积，将10，15变换回320，480（已知输入输出，求解卷积核的w、padding、stride）  \n",
    "根据转置卷积的计算公式，我们已经知道，要放大32倍，那么就把步幅设为32，卷积核至少就要64（能够1/2的尺寸填满这个步长）由于最外面回多出来一圈，所以padding给32/2，这样正好就能在上下/左右消除一个32  \n",
    "一个选取padding的原则就是，要保持size不变的话，padding就是kernel_size/2（偶数）、(kernel_size - 1)/2（奇数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21    #降低到21维是最“便宜的选择”虽然能快一点，但是会带来精度损失\n",
    "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                    kernel_size=64, padding=16, stride=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将图像放大的方法：上采样upsampling  \n",
    "bilinear interpolation，双线性插值是常用的上采样方式之一，可以用于初始化转置卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0625, 0.1875, 0.1875, 0.0625],\n",
       "          [0.1875, 0.5625, 0.5625, 0.1875],\n",
       "          [0.1875, 0.5625, 0.5625, 0.1875],\n",
       "          [0.0625, 0.1875, 0.1875, 0.0625]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0625, 0.1875, 0.1875, 0.0625],\n",
       "          [0.1875, 0.5625, 0.5625, 0.1875],\n",
       "          [0.1875, 0.5625, 0.5625, 0.1875],\n",
       "          [0.0625, 0.1875, 0.1875, 0.0625]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0625, 0.1875, 0.1875, 0.0625],\n",
       "          [0.1875, 0.5625, 0.5625, 0.1875],\n",
       "          [0.1875, 0.5625, 0.5625, 0.1875],\n",
       "          [0.0625, 0.1875, 0.1875, 0.0625]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias=False)\n",
    "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))#用双线性插值函数来初始化权重的话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "img = torchvision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))\n",
    "X = img.unsqueeze(0)\n",
    "Y = conv_trans(X)\n",
    "out_img = Y[0].permute(1, 2, 0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.data.copy_(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, crop_size = 32, (320, 480)\n",
    "train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(inputs, targets):\n",
    "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
    "#得到了的交叉熵还需要在高和宽两个维度做均值\n",
    "num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    X = test_iter.dataset.normalize_image(img).unsqueeze(0)\n",
    "    pred = net(X.to(devices[0])).argmax(dim=1)\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2image(pred):\n",
    "    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\n",
    "    X = pred.long()\n",
    "    return colormap[X, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "n, imgs = 4, []\n",
    "for i in range(n):\n",
    "    crop_rect = (0, 0, 320, 480)\n",
    "    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\n",
    "    pred = label2image(predict(X))\n",
    "    imgs += [X.permute(1,2,0), pred.cpu(),\n",
    "             torchvision.transforms.functional.crop(\n",
    "                 test_labels[i], *crop_rect).permute(1,2,0)]\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
